{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332c648",
   "metadata": {},
   "source": [
    "## 1) Convert dataset from stereo to mono \n",
    "#### We do this because we need the audio files to be in one channel (mono)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8932c93",
   "metadata": {},
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Set input and output directories\n",
    "input_dir = '/Users/cabral/archive/augmented-audio'\n",
    "output_dir = '/Users/cabral/archive/converted-audio'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to convert stereo to mono\n",
    "def convert_to_mono(input_file, output_file):\n",
    "    audio = AudioSegment.from_wav(input_file)\n",
    "    audio = audio.set_channels(1)  # Convert to mono\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "\n",
    "# Walk through the input directory and its subfolders\n",
    "for root, _, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            # Construct the full paths for input and output files\n",
    "            input_file = os.path.join(root, file)\n",
    "            # Create a similar folder structure in the output directory\n",
    "            relative_path = os.path.relpath(input_file, input_dir)\n",
    "            output_file = os.path.join(output_dir, relative_path)\n",
    "\n",
    "            # Create the directory for the output file if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "            # Convert the audio file to mono and save it in the output directory\n",
    "            convert_to_mono(input_file, output_file)\n",
    "            #print(f\"Converted {input_file} to mono and saved to {output_file}\")\n",
    "            \n",
    "\n",
    "print(\"All files have been converted succesfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5121c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce49ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/cabral/archive/converted-audio'\n",
    "data_dir = pathlib.Path(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b157c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Speakers = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "Speakers = Speakers[(Speakers != 'README.md') & (Speakers != '.DS_Store')]\n",
    "print('Speakers:', Speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dae6c",
   "metadata": {},
   "source": [
    "#### The audio clips are 5 seconds long at 44.1 kHz. The output_sequence_length=44100 pads the short ones to exactly 5 seconds (and would trim longer ones) so that they can be easily batched.\n",
    "\n",
    "#### Divided into directories this way, you can easily load the data using: keras.utils.audio_dataset_from_directory. \n",
    "\n",
    "#### Validation_split is set to 0.2, which means that 20% of the data will be used for validation, and the remaining 80% for training.\n",
    "\n",
    "#### Audios are of 5 seconds, so they are 44.1 kHz, which means that every second 44,100 samples were taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=data_dir,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    seed=0,\n",
    "    output_sequence_length=44100,\n",
    "    subset='both')\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print()\n",
    "print(\"label names:\", label_names)\n",
    "\n",
    "for audio, labels in train_ds:\n",
    "    print(\"\\nVerify they are in mono\\nNumber of channels:\", audio.shape[-1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3e82e",
   "metadata": {},
   "source": [
    "#### The dataset now contains batches of audio clips and integer labels. The audio clips have a shape of (batch, samples, channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead351b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dataset only contains single channel audio, \n",
    "#so use the tf.squeeze function to drop the extra axis:\n",
    "\n",
    "def squeeze(audio, labels):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021bec2",
   "metadata": {},
   "source": [
    "#### The utils.audio_dataset_from_directory function only returns up to two splits. It's a good idea to keep a test set separate from your validation set. Ideally you'd keep it in a separate directory, but in this case you can use Dataset.shard to split the validation set into two halves. Note that iterating over any shard will load all the data, and only keep its fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)\n",
    "\n",
    "for example_audio, example_labels in train_ds.take(1):  \n",
    "    print(example_audio.shape)\n",
    "    print(example_labels.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3864cc8",
   "metadata": {},
   "source": [
    "#### Let's plot a few audio waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names[[1,2,3,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f708acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "for i in range(n):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    audio_signal = example_audio[i]\n",
    "    plt.plot(audio_signal)\n",
    "    plt.title(label_names[example_labels[i]])\n",
    "    plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "    plt.ylim([-1.1, 1.1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0da7f",
   "metadata": {},
   "source": [
    "## 2) Convert waveforms to spectrograms\n",
    "#### Keep in mind that CNN works with spectograms which show frequency changes over time and can be represented as 2D images. Feed the spectrogram images into your neural network to train the model.\n",
    "\n",
    "Apply a get_spectrogram function:\n",
    "- This function is used to convert the waveform (time-domain audio) into a spectrogram (frequency-domain representation).\n",
    "- It applies a Short-Time Fourier Transform (STFT) to the waveform, which transforms the audio into a spectrogram.\n",
    "- The shape of the spectrogram is expanded with an extra dimension (tf.newaxis) to be used with convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e59f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "    spectrogram = tf.signal.stft(\n",
    "    waveform, frame_length=255, frame_step=1)\n",
    "    # Obtain the magnitude of the STFT.\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    spectrogram = spectrogram[..., tf.newaxis]\n",
    "    return spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96395a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    label = label_names[example_labels[i]]\n",
    "    waveform = example_audio[i]\n",
    "    spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "    print('Label:', label)\n",
    "    print('Waveform shape:', waveform.shape)\n",
    "    print('Spectrogram shape:', spectrogram.shape)\n",
    "    print('Audio playback')\n",
    "    display.display(display.Audio(waveform, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f1914",
   "metadata": {},
   "source": [
    "#### Define a function to display a spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72399a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "    if len(spectrogram.shape) > 2:\n",
    "        assert len(spectrogram.shape) == 3\n",
    "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f91d8",
   "metadata": {},
   "source": [
    "#### Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 44100])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.suptitle(label.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30cdc9",
   "metadata": {},
   "source": [
    "#### Now, create spectrogram datasets from the audio datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spec_ds(ds):\n",
    "    return ds.map(\n",
    "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
    "      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "test_spectrogram_ds = make_spec_ds(test_ds)\n",
    "\n",
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "    print(\"Hello\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ec125",
   "metadata": {},
   "source": [
    "#### Examine the spectrograms for different examples of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "    print(\"Hello\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678ad0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
